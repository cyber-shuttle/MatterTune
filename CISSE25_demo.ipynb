{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc1b41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -q \"airavata-python-sdk[notebook]\"\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_gpu --file=cybershuttle.yml --walltime=60 --use=expanse:gpu-shared\n",
    "%switch_runtime hpc_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b3106",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import rich\n",
    "\n",
    "import nshutils as nu\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "\n",
    "import mattertune.configs as MC\n",
    "from mattertune import MatterTuner\n",
    "from mattertune.configs import WandbLoggerConfig\n",
    "from mattertune.backbones.jmp.model import get_jmp_s_lr_decay\n",
    "from mattertune.backbones import (\n",
    "    MatterSimM3GNetBackboneModule,\n",
    "    JMPBackboneModule,\n",
    "    ORBBackboneModule,\n",
    "    EqV2BackboneModule,\n",
    ")\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "nu.pretty()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6f476",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def main(args_dict: dict):\n",
    "    def hparams():\n",
    "        hparams = MC.MatterTunerConfig.draft()\n",
    "        \n",
    "        if args_dict[\"model_type\"] == \"mattersim-1m\":\n",
    "            hparams.model = MC.MatterSimBackboneConfig.draft()\n",
    "            hparams.model.graph_convertor = MC.MatterSimGraphConvertorConfig.draft()\n",
    "            hparams.model.pretrained_model = \"MatterSim-v1.0.0-1M\"\n",
    "        elif args_dict[\"model_type\"] == \"jmp-s\":\n",
    "            hparams.model = MC.JMPBackboneConfig.draft()\n",
    "            hparams.model.graph_computer = MC.JMPGraphComputerConfig.draft()\n",
    "            hparams.model.graph_computer.pbc = True\n",
    "            hparams.model.pretrained_model = \"jmp-s\"\n",
    "        elif \"orb\" in args_dict[\"model_type\"]:\n",
    "            hparams.model = MC.ORBBackboneConfig.draft()\n",
    "            hparams.model.pretrained_model = args_dict[\"model_type\"]\n",
    "        elif args_dict[\"model_type\"] == \"eqv2\":\n",
    "            hparams.model = MC.EqV2BackboneConfig.draft()\n",
    "            hparams.model.checkpoint_path = Path(\n",
    "                \"/net/csefiles/coc-fung-cluster/nima/shared/checkpoints/eqV2_31M_mp.pt\"\n",
    "            )\n",
    "            hparams.model.atoms_to_graph = MC.FAIRChemAtomsToGraphSystemConfig.draft()\n",
    "            hparams.model.atoms_to_graph.radius = 8.0\n",
    "            hparams.model.atoms_to_graph.max_num_neighbors = 20\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid model type, please choose from ['mattersim-1m', 'jmp-s', 'orb-v2']\"\n",
    "            )\n",
    "        hparams.model.ignore_gpu_batch_transform_error = True\n",
    "        hparams.model.freeze_backbone = False\n",
    "        hparams.model.reset_output_heads = True\n",
    "        hparams.model.optimizer = MC.AdamWConfig(\n",
    "            lr=8.0e-5,\n",
    "            amsgrad=False,\n",
    "            betas=(0.9, 0.95),\n",
    "            eps=1.0e-8,\n",
    "            weight_decay=0.1,\n",
    "            per_parameter_hparams=get_jmp_s_lr_decay(args_dict[\"lr\"]) if \"jmp\" in args_dict[\"model_type\"] else None,\n",
    "        )\n",
    "        if args_dict[\"lr_scheduler\"] == \"steplr\":\n",
    "            hparams.model.lr_scheduler = MC.StepLRConfig(\n",
    "                step_size=10, gamma=0.9\n",
    "            )\n",
    "        elif args_dict[\"lr_scheduler\"] == \"rlp\":\n",
    "            hparams.model.lr_scheduler = MC.ReduceOnPlateauConfig(\n",
    "                mode=\"min\",\n",
    "                monitor=f\"val/forces_mae\",\n",
    "                factor=0.8,\n",
    "                patience=5,\n",
    "                min_lr=1e-8,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid lr_scheduler, please choose from ['steplr', 'rlp']\"\n",
    "            )\n",
    "        hparams.trainer.ema = MC.EMAConfig(decay=args_dict[\"ema_decay\"])\n",
    "\n",
    "        # Add model properties\n",
    "        hparams.model.properties = []\n",
    "        energy_coefficient = 1.0 \n",
    "        conservative = args_dict[\"conservative\"] or \"mattersim\" in args_dict[\"model_type\"]\n",
    "        energy = MC.EnergyPropertyConfig(\n",
    "            loss=MC.MSELossConfig(), loss_coefficient=energy_coefficient\n",
    "        )\n",
    "        hparams.model.properties.append(energy)\n",
    "        forces = MC.ForcesPropertyConfig(\n",
    "            loss=MC.MSELossConfig(), conservative=conservative, loss_coefficient=1.0\n",
    "        )\n",
    "        hparams.model.properties.append(forces)\n",
    "\n",
    "        ## Data Hyperparameters\n",
    "        hparams.data = MC.ManualSplitDataModuleConfig.draft()\n",
    "        hparams.data.train = MC.XYZDatasetConfig.draft()\n",
    "        hparams.data.train.src = \"./data/train_water_1000_eVAng.xyz\"\n",
    "        hparams.data.train.down_sample = args_dict[\"train_down_sample\"]\n",
    "        hparams.data.train.down_sample_refill = args_dict[\"down_sample_refill\"]\n",
    "        hparams.data.validation = MC.XYZDatasetConfig.draft()\n",
    "        hparams.data.validation.src = \"./data/val_water_1000_eVAng.xyz\"\n",
    "        hparams.data.batch_size = args_dict[\"batch_size\"]\n",
    "\n",
    "        ## Add Normalization for Energy\n",
    "        hparams.model.normalizers = {\n",
    "            \"energy\": [\n",
    "                MC.PerAtomReferencingNormalizerConfig(\n",
    "                    per_atom_references=Path(\"./data/water_1000_eVAng-energy_reference.json\")\n",
    "                ),\n",
    "                MC.PerAtomNormalizerConfig(),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        ## Trainer Hyperparameters\n",
    "        hparams.trainer = MC.TrainerConfig.draft()\n",
    "        hparams.trainer.max_epochs = args_dict[\"max_epochs\"]\n",
    "        hparams.trainer.accelerator = \"gpu\"\n",
    "        hparams.trainer.devices = args_dict[\"devices\"]\n",
    "        hparams.trainer.strategy = DDPStrategy(find_unused_parameters=True) if not \"orb\" in args_dict[\"model_type\"] else DDPStrategy(static_graph=True, find_unused_parameters=True)\n",
    "        hparams.trainer.gradient_clip_algorithm = \"norm\"\n",
    "        hparams.trainer.gradient_clip_val = 1.0\n",
    "        hparams.trainer.precision = \"32\"\n",
    "\n",
    "        # Configure Early Stopping\n",
    "        hparams.trainer.early_stopping = MC.EarlyStoppingConfig(\n",
    "            monitor=f\"val/forces_mae\", patience=50, mode=\"min\"\n",
    "        )\n",
    "\n",
    "        # Configure Model Checkpoint\n",
    "        ckpt_name = f\"{args_dict['model_type']}-best-{args_dict['train_down_sample']}\"\n",
    "        if args_dict[\"down_sample_refill\"]:\n",
    "            ckpt_name += \"-refill\"\n",
    "        if args_dict[\"conservative\"]:\n",
    "            ckpt_name += \"-conservative\"\n",
    "        hparams.trainer.checkpoint = MC.ModelCheckpointConfig(\n",
    "            monitor=\"val/forces_mae\",\n",
    "            dirpath=\"./checkpoints\",\n",
    "            filename=ckpt_name,\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "            every_n_epochs=10,\n",
    "        )\n",
    "\n",
    "        # Configure Logger\n",
    "        hparams.trainer.loggers = [\n",
    "            WandbLoggerConfig(\n",
    "                project=\"MatterTune-Water-Finetune\",\n",
    "                name=f\"Water-{args_dict['model_type']}-{args_dict['train_down_sample']}-refill_{args_dict['down_sample_refill']}\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # Additional trainer settings\n",
    "        hparams.trainer.additional_trainer_kwargs = {\n",
    "            \"inference_mode\": False,\n",
    "        }\n",
    "\n",
    "        hparams = hparams.finalize(strict=False)\n",
    "        return hparams\n",
    "\n",
    "    mt_config = hparams()\n",
    "    model, trainer = MatterTuner(mt_config).tune()\n",
    "    \n",
    "    \n",
    "    ## Perform Evaluation\n",
    "\n",
    "    ckpt_path = f\"./checkpoints/{args_dict['model_type']}-best-{args_dict['train_down_sample']}\"\n",
    "    if args_dict[\"down_sample_refill\"]:\n",
    "        ckpt_path += \"-refill\"\n",
    "    if args_dict[\"conservative\"]:\n",
    "        ckpt_path += \"-conservative\"\n",
    "    ckpt_path += \".ckpt\"\n",
    "    \n",
    "    if \"mattersim\" in args_dict[\"model_type\"]:\n",
    "        ft_model = MatterSimM3GNetBackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    elif \"jmp\" in args_dict[\"model_type\"]:\n",
    "        ft_model = JMPBackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    elif \"orb\" in args_dict[\"model_type\"]:\n",
    "        ft_model = ORBBackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    elif \"eqv2\" in args_dict[\"model_type\"]:\n",
    "        ft_model = EqV2BackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid model type, please choose from ['mattersim-1m', 'jmp-s', 'orb-v2', 'eqv2']\"\n",
    "        )\n",
    "    \n",
    "    from ase.io import read\n",
    "    from ase import Atoms\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import wandb\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    wandb.init(project=\"MatterTune-Water-Finetune\", name=f\"Water-{args_dict['model_type']}-{args_dict['train_down_sample']}-refill_{args_dict['down_sample_refill']}\", resume=\"allow\")\n",
    "    \n",
    "    val_atoms_list:list[Atoms] = read(\"./data/val_water_1000_eVAng.xyz\", \":\") # type: ignore\n",
    "    calc = ft_model.ase_calculator(\n",
    "        device = f\"cuda:{args_dict['devices'][0]}\"\n",
    "    )\n",
    "    energies_per_atom = []\n",
    "    forces = []\n",
    "    pred_energies_per_atom = []\n",
    "    pred_forces = []\n",
    "    for atoms in tqdm(val_atoms_list):\n",
    "        energies_per_atom.append(atoms.get_potential_energy() / len(atoms))\n",
    "        forces.extend(np.array(atoms.get_forces()).tolist())\n",
    "        atoms.set_calculator(calc)\n",
    "        pred_energies_per_atom.append(atoms.get_potential_energy() / len(atoms))\n",
    "        pred_forces.extend(np.array(atoms.get_forces()).tolist())\n",
    "        \n",
    "    e_mae = torch.nn.L1Loss()(torch.tensor(energies_per_atom), torch.tensor(pred_energies_per_atom))\n",
    "    f_mae = torch.nn.L1Loss()(torch.tensor(forces), torch.tensor(pred_forces))\n",
    "    \n",
    "    rich.print(f\"Energy MAE: {e_mae} eV/atom\")\n",
    "    rich.print(f\"Forces MAE: {f_mae} eV/Ang\")\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b02ee0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model_type\", type=str, default=\"mattersim-1m\")\n",
    "parser.add_argument(\"--conservative\", action=\"store_true\")\n",
    "parser.add_argument(\"--train_down_sample\", type=int, default=30)\n",
    "parser.add_argument(\"--down_sample_refill\", type=bool, default=True)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=16)\n",
    "parser.add_argument(\"--lr\", type=float, default=8e-5)\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=1000)\n",
    "parser.add_argument(\"--devices\", type=int, nargs=\"+\", default=[0, 1, 2, 3])\n",
    "parser.add_argument(\"--lr_scheduler\", type=str, default=\"steplr\")\n",
    "parser.add_argument(\"--ema_decay\", type=float, default=0.99)\n",
    "args = parser.parse_args()\n",
    "args_dict = vars(args)\n",
    "\n",
    "main(args_dict)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
