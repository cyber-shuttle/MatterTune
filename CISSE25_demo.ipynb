{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46887664-4eb5-4757-b2da-1a686fedb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U \"airavata-python-sdk[notebook]\"\n",
    "%pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edc1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_gpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC2:cloud\n",
    "%wait_for_runtime hpc_gpu\n",
    "%switch_runtime hpc_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd9d6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --force-reinstall --no-deps --no-cache-dir git+https://github.com/cyber-shuttle/MatterTune.git\n",
    "%pip install -U ipywidgets rich nshconfig-extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15682514-4c7d-4caf-a414-682f36ef5d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32b320-cc89-4f95-9d1c-bff7b4c14dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/cyber-shuttle/MatterTune/refs/heads/main/examples/water-thermodynamics/data/train_water_1000_eVAng.xyz -O train_water_1000_eVAng.xyz\n",
    "! wget https://raw.githubusercontent.com/cyber-shuttle/MatterTune/refs/heads/main/examples/water-thermodynamics/data/val_water_1000_eVAng.xyz -O val_water_1000_eVAng.xyz\n",
    "! wget https://raw.githubusercontent.com/cyber-shuttle/MatterTune/refs/heads/main/examples/water-thermodynamics/data/H2O.xyz -O H2O.xyz\n",
    "! wget https://raw.githubusercontent.com/cyber-shuttle/MatterTune/refs/heads/main/examples/water-thermodynamics/data/water_1000_eVAng-energy_reference.json -O water_1000_eVAng-energy_reference.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import rich\n",
    "import nshutils as nu\n",
    "import nshconfig as C\n",
    "import nshconfig_extra as CE\n",
    "\n",
    "from ase.io import read\n",
    "from ase import Atoms\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mattertune.configs as MC\n",
    "from mattertune import MatterTuner\n",
    "from mattertune.backbones import (\n",
    "    MatterSimM3GNetBackboneModule,\n",
    "    JMPBackboneModule,\n",
    "    ORBBackboneModule,\n",
    "    EqV2BackboneModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d6f476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune(args_dict: dict):\n",
    "    \"\"\"\n",
    "    Fine-tune a Pre-trained Atomistic Foundation Model using MatterTuner.\n",
    "    - args_dict: Dictionary containing hyperparameters and configurations for fine-tuning.\n",
    "    \"\"\"\n",
    "    def hparams():\n",
    "        hparams = MC.MatterTunerConfig.draft()\n",
    "\n",
    "        ## Choose Backbone Model type\n",
    "        if args_dict[\"model_type\"] == \"mattersim-1m\":\n",
    "            hparams.model = MC.MatterSimBackboneConfig.draft()\n",
    "            hparams.model.graph_convertor = MC.MatterSimGraphConvertorConfig.draft()\n",
    "            hparams.model.pretrained_model = \"MatterSim-v1.0.0-1M\"\n",
    "        elif args_dict[\"model_type\"] == \"jmp-s\":\n",
    "            hparams.model = MC.JMPBackboneConfig.draft()\n",
    "            hparams.model.graph_computer = MC.JMPGraphComputerConfig.draft()\n",
    "            hparams.model.graph_computer.pbc = True\n",
    "            hparams.model.pretrained_model = \"jmp-s\"\n",
    "        elif \"orb\" in args_dict[\"model_type\"]:\n",
    "            hparams.model = MC.ORBBackboneConfig.draft()\n",
    "            hparams.model.pretrained_model = args_dict[\"model_type\"]\n",
    "        elif args_dict[\"model_type\"] == \"eqv2\":\n",
    "            hparams.model = MC.EqV2BackboneConfig.draft()\n",
    "            hparams.model.checkpoint_path = CE.CachedPathConfig(uri=\"hf://facebook/OMAT24/eqV2_31M_mp.pt\").resolve()\n",
    "            hparams.model.atoms_to_graph = MC.FAIRChemAtomsToGraphSystemConfig.draft()\n",
    "            hparams.model.atoms_to_graph.radius = 8.0\n",
    "            hparams.model.atoms_to_graph.max_num_neighbors = 20\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid model type, please choose from ['mattersim-1m', 'jmp-s', 'orb-v2']\"\n",
    "            )\n",
    "        hparams.model.reset_output_heads = True\n",
    "\n",
    "        ## Set Hyperparameters of optimizer and scheduler\n",
    "        hparams.model.optimizer = MC.AdamWConfig(\n",
    "            lr=args_dict[\"lr\"],\n",
    "            amsgrad=False,\n",
    "            betas=(0.9, 0.95),\n",
    "            eps=1.0e-8,\n",
    "            weight_decay=0.1,\n",
    "        )\n",
    "        hparams.model.lr_scheduler = MC.ReduceOnPlateauConfig(\n",
    "            mode=\"min\",\n",
    "            monitor=f\"val/forces_mae\",\n",
    "            factor=0.8,\n",
    "            patience=5,\n",
    "            min_lr=1e-8,\n",
    "        )\n",
    "        hparams.trainer.ema = MC.EMAConfig(decay=0.99)\n",
    "\n",
    "        # Add model properties, these are the properties to be predicted by the model\n",
    "        hparams.model.properties = []\n",
    "        energy_coefficient = 1.0\n",
    "        conservative = args_dict[\"conservative\"] or \"mattersim\" in args_dict[\"model_type\"]\n",
    "        energy = MC.EnergyPropertyConfig(\n",
    "            loss=MC.MSELossConfig(), loss_coefficient=energy_coefficient\n",
    "        )\n",
    "        hparams.model.properties.append(energy)\n",
    "        forces = MC.ForcesPropertyConfig(\n",
    "            loss=MC.MSELossConfig(), conservative=conservative, loss_coefficient=1.0\n",
    "        )\n",
    "        hparams.model.properties.append(forces)\n",
    "\n",
    "        ## Set Data Module to load the dataset\n",
    "        ## Here we downsampled 30 data points from the original training set because it's already enough\n",
    "        hparams.data = MC.ManualSplitDataModuleConfig.draft()\n",
    "        hparams.data.train = MC.XYZDatasetConfig.draft()\n",
    "        hparams.data.train.src = \"./train_water_1000_eVAng.xyz\"\n",
    "        hparams.data.train.down_sample = 30\n",
    "        hparams.data.train.down_sample_refill = True ### Although we only used 30 samples, we repeate them to reach the original dataset size\n",
    "        hparams.data.validation = MC.XYZDatasetConfig.draft()\n",
    "        hparams.data.validation.src = \"./val_water_1000_eVAng.xyz\"\n",
    "        hparams.data.batch_size = args_dict[\"batch_size\"]\n",
    "        hparams.data.num_workers = 0\n",
    "\n",
    "        ## Normalization usually helps with training stability and convergence\n",
    "        ## Here we normalize the energy firstly by linear referencing and then divide by number of atoms\n",
    "        hparams.model.normalizers = {\n",
    "            \"energy\": [\n",
    "                MC.PerAtomReferencingNormalizerConfig(\n",
    "                    per_atom_references=Path(\"./water_1000_eVAng-energy_reference.json\")\n",
    "                ),\n",
    "                MC.PerAtomNormalizerConfig(),\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        ## Trainer Hyperparameters, used to configure number of epochs, devices, etc.\n",
    "        hparams.trainer = MC.TrainerConfig.draft()\n",
    "        hparams.trainer.max_epochs = args_dict[\"max_epochs\"]\n",
    "        hparams.trainer.accelerator = \"gpu\"\n",
    "        hparams.trainer.devices = args_dict[\"devices\"]\n",
    "        hparams.trainer.gradient_clip_algorithm = \"norm\"\n",
    "        hparams.trainer.gradient_clip_val = 1.0\n",
    "        hparams.trainer.precision = \"32\"\n",
    "\n",
    "        ## Configure Early Stopping\n",
    "        hparams.trainer.early_stopping = MC.EarlyStoppingConfig(\n",
    "            monitor=f\"val/forces_mae\", patience=50, mode=\"min\"\n",
    "        )\n",
    "\n",
    "        ## Configure Model Checkpoint\n",
    "        hparams.trainer.checkpoint = MC.ModelCheckpointConfig(\n",
    "            monitor=\"val/forces_mae\",\n",
    "            dirpath=\"./checkpoints\",\n",
    "            filename=f\"{args_dict['model_type']}-best\",\n",
    "            save_top_k=1,\n",
    "            mode=\"min\",\n",
    "        )\n",
    "\n",
    "        ## Configure Logger\n",
    "        hparams.trainer.loggers = [\n",
    "            MC.TensorBoardLoggerConfig(\n",
    "                save_dir=\"./logs\",\n",
    "                name=f\"{args_dict['model_type']}-tune\",\n",
    "                version=0,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "        ## Additional trainer settings\n",
    "        ## Here since mattersim models are conservative, we set inference_mode to False to enable differentiable energy prediction\n",
    "        from lightning.pytorch.callbacks import RichProgressBar\n",
    "        hparams.trainer.additional_trainer_kwargs = {\n",
    "            \"inference_mode\": False,\n",
    "            \"callbacks\": [RichProgressBar()],\n",
    "        }\n",
    "\n",
    "\n",
    "        hparams = hparams.finalize(strict=False)\n",
    "        return hparams\n",
    "\n",
    "    mt_config = hparams()\n",
    "    model, trainer = MatterTuner(mt_config).tune()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b89186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(args_dict: dict):\n",
    "    \"\"\"\n",
    "    Perform inference using the fine-tuned model on the validation dataset.\n",
    "    Load the fine-tuned model from the checkpoint using .load_from_checkpoint function.\n",
    "    Convert the loaded model to an ASE calculator using the `ase_calculator` method.\n",
    "    Evaluate the model on the validation dataset and compute the MAE for energies and forces.\n",
    "    \"\"\"\n",
    "\n",
    "    ckpt_path = f\"./checkpoints/{args_dict['model_type']}-best.ckpt\"\n",
    "    if \"mattersim\" in args_dict[\"model_type\"]:\n",
    "        ft_model = MatterSimM3GNetBackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    elif \"jmp\" in args_dict[\"model_type\"]:\n",
    "        ft_model = JMPBackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    elif \"orb\" in args_dict[\"model_type\"]:\n",
    "        ft_model = ORBBackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    elif \"eqv2\" in args_dict[\"model_type\"]:\n",
    "        ft_model = EqV2BackboneModule.load_from_checkpoint(ckpt_path)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid model type, please choose from ['mattersim-1m', 'jmp-s', 'orb-v2', 'eqv2']\"\n",
    "        )\n",
    "\n",
    "    val_atoms_list:list[Atoms] = read(\"./val_water_1000_eVAng.xyz\", \":\") # type: ignore\n",
    "    calc = ft_model.ase_calculator(\n",
    "        device = f\"cuda:{args_dict['devices'][0]}\"\n",
    "    )\n",
    "    energies_per_atom = []\n",
    "    forces = []\n",
    "    pred_energies_per_atom = []\n",
    "    pred_forces = []\n",
    "    for atoms in tqdm(val_atoms_list):\n",
    "        energies_per_atom.append(atoms.get_potential_energy() / len(atoms))\n",
    "        forces.extend(np.array(atoms.get_forces()).tolist())\n",
    "        atoms.set_calculator(calc)\n",
    "        pred_energies_per_atom.append(atoms.get_potential_energy() / len(atoms))\n",
    "        pred_forces.extend(np.array(atoms.get_forces()).tolist())\n",
    "\n",
    "    e_mae = torch.nn.L1Loss()(torch.tensor(energies_per_atom), torch.tensor(pred_energies_per_atom))\n",
    "    f_mae = torch.nn.L1Loss()(torch.tensor(forces), torch.tensor(pred_forces))\n",
    "\n",
    "    rich.print(f\"Energy MAE: {e_mae} eV/atom\")\n",
    "    rich.print(f\"Forces MAE: {f_mae} eV/Ang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73862e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We support various logging methods, including TensorBoard, WandB, and CSV files.\n",
    "## In this demo we used TensorBoard, so we can visualize the training process using TensorBoard.\n",
    "## Below is a script to visualize the training loss using TensorBoard logs.\n",
    "\n",
    "def visualize_tensorboard_logs(metric_name: str, unit: str):\n",
    "    from tensorboard.backend.event_processing import event_accumulator\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Read the event file\n",
    "    ea = event_accumulator.EventAccumulator(\"./logs/mattersim-1m-tune/version_0\")\n",
    "    ea.Reload()\n",
    "\n",
    "    # Check what scalar tags are available\n",
    "    print(ea.Tags()[\"scalars\"])\n",
    "\n",
    "    # For example, we export the 'loss' curve\n",
    "    scalar_events = ea.Scalars(metric_name)\n",
    "    steps = [e.step for e in scalar_events]\n",
    "    values = [e.value for e in scalar_events]\n",
    "\n",
    "    # Plotting\n",
    "    plt.plot(steps, values)\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.title(f\"{metric_name} ({unit}) over steps\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b02ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"model_type\": \"mattersim-1m\",\n",
    "    \"conservative\": True,\n",
    "    \"batch_size\": 2,\n",
    "    \"max_epochs\": 2,\n",
    "    \"lr\": 8e-5,\n",
    "    \"devices\": [0],\n",
    "}\n",
    "fine_tune(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89cf956",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tensorboard_logs(\"val/forces_mae\", unit=\"eV/Ang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18fa42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Further we can use the fine-tuned model to run Molecular Dynamics (MD) simulations.\n",
    "from ase.md.langevin import Langevin\n",
    "import ase.units as units\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "atoms:Atoms = read(\"./H2O.xyz\")  # type: ignore\n",
    "atoms.pbc = True\n",
    "ft_model = MatterSimM3GNetBackboneModule.load_from_checkpoint(\"./checkpoints/mattersim-1m-best.ckpt\")\n",
    "calc = ft_model.ase_calculator()\n",
    "atoms.calc = calc\n",
    "\n",
    "dyn = Langevin(\n",
    "    atoms,\n",
    "    temperature_K=300,\n",
    "    timestep=0.5 * units.fs,\n",
    "    friction=0.02,\n",
    ")\n",
    "pbar = tqdm(total=100, desc=\"MD Simulation Steps\")\n",
    "temperatures = []\n",
    "for step in range(100):\n",
    "    dyn.step()\n",
    "    temp = atoms.get_temperature()\n",
    "    pbar.set_description(f\"MD Simulation Steps (Temperature: {temp:.2f} K/{300:.2f} K)\")\n",
    "    pbar.update(1)\n",
    "    temperatures.append(temp)\n",
    "\n",
    "plt.plot(temperatures)\n",
    "plt.xlabel(\"steps\")\n",
    "plt.ylabel(\"temperature (K)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9869904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
